{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe2ec573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Input, Reshape, Conv2D, ReLU, MaxPool2D, Masking\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import ast\n",
    "\n",
    "# Load and preprocess the data\n",
    "# Replace 'train.csv' with the actual path to your dataset\n",
    "data = pd.read_csv('dataset/combined_shuffled.csv', header = None, converters = {\n",
    "    1: ast.literal_eval,\n",
    "    2: ast.literal_eval,\n",
    "    3: ast.literal_eval\n",
    "})\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['acc_x'] = data[1]\n",
    "df['acc_y'] = data[2]\n",
    "df['acc_z'] = data[3]\n",
    "df['gesture'] = data[0]\n",
    "\n",
    "#remove invalid rows\n",
    "df.drop(df.loc[df['acc_x']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_y']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_z']==0].index, inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert the lists into arrays\n",
    "acc_x = df['acc_x'].values\n",
    "acc_y = df['acc_y'].values\n",
    "acc_z = df['acc_z'].values\n",
    "\n",
    "# Combine all axes into a sequence of shape (timesteps, features)\n",
    "sequences = [np.array([x, y, z]).T for x, y, z in zip(acc_x, acc_y, acc_z)]\n",
    "\n",
    "# Pad sequences to the length of the longest sequence\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 32, padding='post', dtype='float32')\n",
    "\n",
    "# Encode labels\n",
    "labels = df['gesture'].values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebaa9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_labels = categorical_labels\n",
    "initial_sequences = padded_sequences\n",
    "\n",
    "#DATA AUGMENTATION preprocessing\n",
    "\n",
    "#adding noise\n",
    "def add_noise(data, noise_level=0.05):\n",
    "    return (data + np.random.normal(0, noise_level, data.shape)).astype(np.float32)\n",
    "\n",
    "# Original data: `x_train` (accelerometer sequences), `y_train` (labels)\n",
    "\n",
    "noised_data = add_noise(initial_sequences)\n",
    "\n",
    "def scale_data(data, scaling_factor=0.1):\n",
    "    \n",
    "    factor = 1 + np.random.uniform(-scaling_factor, scaling_factor)\n",
    "    return (data * factor).astype(np.float32)\n",
    "\n",
    "def time_shift(data, max_shift_percentage=0.1):\n",
    "  \n",
    "  shift_amount = int(len(data) * max_shift_percentage * (2 * np.random.rand() - 1))\n",
    "  shifted_data = np.roll(data, shift_amount, axis=0)\n",
    "  return (shifted_data).astype(np.float32)\n",
    "\n",
    "#categorical_labels = np.concatenate((categorical_labels, initial_labels))\n",
    "\n",
    "\"\"\"def time_warp(data, warping_factor=0.1):\n",
    "    \n",
    "  timesteps = np.arange(len(data))\n",
    "  warped_timesteps = timesteps + warping_factor * (np.random.rand(len(data)) - 0.5) * len(data)\n",
    "  warped_timesteps = np.interp(np.arange(len(data)), np.sort(warped_timesteps), timesteps)\n",
    "  warped_data = np.zeros_like(data)\n",
    "  for i in range(len(data)):\n",
    "    warped_data[i] = np.interp(i, timesteps, data)\n",
    "  return warped_data\n",
    "  \"\"\"\n",
    "\n",
    "#warped_seq = time_warp(padded_sequences)\n",
    "\n",
    "def rotate_3d(data, max_angle_degrees=10):\n",
    "\n",
    "  angle_x = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "  angle_y = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "  angle_z = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "\n",
    "  rotation_x = np.array([[1, 0, 0],\n",
    "                        [0, np.cos(angle_x), -np.sin(angle_x)],\n",
    "                        [0, np.sin(angle_x), np.cos(angle_x)]])\n",
    "  rotation_y = np.array([[np.cos(angle_y), 0, np.sin(angle_y)],\n",
    "                        [0, 1, 0],\n",
    "                        [-np.sin(angle_y), 0, np.cos(angle_y)]])\n",
    "  rotation_z = np.array([[np.cos(angle_z), -np.sin(angle_z), 0],\n",
    "                        [np.sin(angle_z), np.cos(angle_z), 0],\n",
    "                        [0, 0, 1]])\n",
    "\n",
    "  rotation_matrix = np.dot(np.dot(rotation_x, rotation_y), rotation_z)\n",
    "  rotated_data = np.dot(data, rotation_matrix.T)\n",
    "\n",
    "  return (rotated_data).astype(np.float32)\n",
    "\n",
    "rotated_seq = rotate_3d(initial_sequences)\n",
    "\n",
    "padded_sequences = np.concatenate((padded_sequences, add_noise(padded_sequences)))\n",
    "categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "#padded_sequences = np.concatenate((padded_sequences, scale_data(padded_sequences)))\n",
    "#categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "#padded_sequences = np.concatenate((padded_sequences, time_shift(padded_sequences)))\n",
    "#categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "padded_sequences = np.concatenate((padded_sequences, rotate_3d(padded_sequences)))\n",
    "categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c22ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    padded_sequences, categorical_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81763a33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 1s 4ms/step - loss: 1.9254 - accuracy: 0.2626 - val_loss: 1.7208 - val_accuracy: 0.5882\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.6363 - accuracy: 0.4224 - val_loss: 1.5236 - val_accuracy: 0.7911\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.5062 - accuracy: 0.4881 - val_loss: 1.3425 - val_accuracy: 0.8662\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.3830 - accuracy: 0.5469 - val_loss: 1.1976 - val_accuracy: 0.8849\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.3067 - accuracy: 0.5529 - val_loss: 1.0560 - val_accuracy: 0.8951\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.2217 - accuracy: 0.5839 - val_loss: 0.9364 - val_accuracy: 0.9113\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.1377 - accuracy: 0.6174 - val_loss: 0.7854 - val_accuracy: 0.9199\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.0887 - accuracy: 0.6327 - val_loss: 0.7055 - val_accuracy: 0.9224\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.0472 - accuracy: 0.6418 - val_loss: 0.5659 - val_accuracy: 0.9301\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.0103 - accuracy: 0.6472 - val_loss: 0.5161 - val_accuracy: 0.9395\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9632 - accuracy: 0.6717 - val_loss: 0.4543 - val_accuracy: 0.9369\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9366 - accuracy: 0.6737 - val_loss: 0.4299 - val_accuracy: 0.9437\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9362 - accuracy: 0.6686 - val_loss: 0.3766 - val_accuracy: 0.9463\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8689 - accuracy: 0.6907 - val_loss: 0.3322 - val_accuracy: 0.9471\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.8406 - accuracy: 0.6998 - val_loss: 0.3148 - val_accuracy: 0.9471\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8661 - accuracy: 0.6930 - val_loss: 0.2925 - val_accuracy: 0.9574\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8238 - accuracy: 0.7064 - val_loss: 0.2758 - val_accuracy: 0.9582\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8310 - accuracy: 0.6995 - val_loss: 0.2587 - val_accuracy: 0.9557\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7828 - accuracy: 0.7140 - val_loss: 0.2490 - val_accuracy: 0.9616\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7832 - accuracy: 0.7112 - val_loss: 0.2407 - val_accuracy: 0.9599\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9761\n",
      "Test Loss: 0.2177741527557373, Test Accuracy: 0.9761295914649963\n",
      "37/37 [==============================] - 0s 903us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neural network model\n",
    "model = Sequential([\n",
    "    Input(shape=(32, 3)),\n",
    "    # Mask padding values\n",
    "    #Masking(mask_value=0.0, input_shape=(None, 3)),\n",
    "    Reshape((8,4,3)),\n",
    "    Conv2D(filters=16, kernel_size = (3,3), padding = \"same\"),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    Conv2D(filters=8, kernel_size = (3,3), padding= \"same\"),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPool2D(pool_size = 2),\n",
    "    \n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(8),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "\n",
    "    Dense(len(label_encoder.classes_), activation = 'softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_gesture_classification_model_demo.h5')\n",
    "\n",
    "# Decode predicted labels for interpretability\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(model.predict(X_test), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d07945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpeq63t1qj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpeq63t1qj/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with full integer quantization saved as gesture_classification_model_integer_demo.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-08 10:51:06.421172: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-08 10:51:06.421188: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-08 10:51:06.421401: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpeq63t1qj\n",
      "2025-01-08 10:51:06.422994: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-08 10:51:06.423005: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpeq63t1qj\n",
      "2025-01-08 10:51:06.425757: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2025-01-08 10:51:06.427004: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-08 10:51:06.475685: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpeq63t1qj\n",
      "2025-01-08 10:51:06.488572: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 67170 microseconds.\n",
      "2025-01-08 10:51:06.517140: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 14, Total Ops 28, % non-converted = 50.00 %\n",
      " * 14 ARITH ops\n",
      "\n",
      "- arith.constant:   14 occurrences  (f32: 8, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(padded_sequences).batch(1).take(32):\n",
    "    yield [input_value]\n",
    "        \n",
    "# Ensure input shape is fixed\n",
    "model.build(input_shape=(None, 32, 3))  # Example: Fixed length 32 timesteps, 3 features\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Apply integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # Ensure compatibility\n",
    "converter.inference_input_type = tf.uint8  # Optional: Set input type\n",
    "converter.inference_output_type = tf.uint8  # Optional: Set output type\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('cnn_gesture_classification_model_integer_demo.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model with full integer quantization saved as gesture_classification_model_integer_demo.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fc5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptxzjnljn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptxzjnljn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 Model size without quantization: 98 KB\n",
      "TFLite Model size without quantization: 13 KB\n",
      "\n",
      "Reduction in file size by a factor of 7.382456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 10:51:10.000312: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-08 10:51:10.000328: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-08 10:51:10.000430: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmptxzjnljn\n",
      "2025-01-08 10:51:10.002001: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-08 10:51:10.002016: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmptxzjnljn\n",
      "2025-01-08 10:51:10.006015: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-08 10:51:10.050892: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmptxzjnljn\n",
      "2025-01-08 10:51:10.062596: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 62166 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 14, Total Ops 28, % non-converted = 50.00 %\n",
      " * 14 ARITH ops\n",
      "\n",
      "- arith.constant:   14 occurrences  (f32: 8, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Convert the model to TFLite without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "fp_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"cnn_model_demo_f32.tflite\", \"wb\").write(fp_tflite_model)\n",
    "\n",
    "# Show the model size for the non-quantized HDF5 model\n",
    "fp_h5_in_kb = os.path.getsize('models/cnn_gesture_classification_model_demo.h5') / 1024\n",
    "print(\"HDF5 Model size without quantization: %d KB\" % fp_h5_in_kb)\n",
    "\n",
    "# Show the model size for the non-quantized TFLite model\n",
    "fp_tflite_in_kb = os.path.getsize('cnn_model_demo_f32.tflite') / 1024\n",
    "print(\"TFLite Model size without quantization: %d KB\" % fp_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in file size by a factor of %f\" % (fp_h5_in_kb / fp_tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90af0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n",
      "TFLite Model size with 8-bit quantization: 8 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#19 is a dynamic-sized tensor).\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "\n",
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('models/cnn_gesture_classification_model_integer_demo.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ff3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "\n",
    "test_sequence_indices = range(X_test.shape[0])\n",
    "\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global X_test\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = []\n",
    "  for i, test_sequence_index in enumerate(test_sequence_indices):\n",
    "    test_sequence = X_test[test_sequence_index]\n",
    "    test_label = y_test[test_sequence_index]\n",
    "\n",
    "    if (test_sequence_index % 100 == 0):\n",
    "      print(\"Evaluated on %d sequences.\" % test_sequence_index)\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_sequence = test_sequence / input_scale + input_zero_point\n",
    "\n",
    "    test_sequence = np.expand_dims(test_sequence, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_sequence)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions.append(np.argmax(output, axis=-1))\n",
    "\n",
    "  return predictions\n",
    "\n",
    "\n",
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  test_sequence_indices = range(X_train.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_sequence_indices)\n",
    "    \n",
    "  labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "  accuracy = (np.sum(labels == predictions) * 100) / len(X_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(X_test)))\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "scale, zero_point = input_details[\"quantization\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60907cd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 sequences.\n",
      "Evaluated on 100 sequences.\n",
      "Evaluated on 200 sequences.\n",
      "Evaluated on 300 sequences.\n",
      "Evaluated on 400 sequences.\n",
      "Evaluated on 500 sequences.\n",
      "Evaluated on 600 sequences.\n",
      "Evaluated on 700 sequences.\n",
      "Evaluated on 800 sequences.\n",
      "Evaluated on 900 sequences.\n",
      "Evaluated on 1000 sequences.\n",
      "Evaluated on 1100 sequences.\n",
      "Full Post-Quantized INT8 model accuracy is 97.1014% (Number of test samples=1173)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "# this might take a few minutes (~ 1- 2 minutes)\n",
    "# if it takes longer than that, I suggest to \n",
    "# restart the runtime and try again\n",
    "# if the issue still persists, restart your computer\n",
    "tflite_model_quant_int8_file = pathlib.Path('models/cnn_gesture_classification_model_integer_demo.tflite')\n",
    "tflite_model_quant_int8_model_type = \"Full Post-Quantized INT8\"\n",
    "\n",
    "evaluate_model(tflite_model_quant_int8_file, tflite_model_quant_int8_model_type)\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5cb06e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 sequences.\n",
      "Evaluated on 100 sequences.\n",
      "Evaluated on 200 sequences.\n",
      "Evaluated on 300 sequences.\n",
      "Evaluated on 400 sequences.\n",
      "Evaluated on 500 sequences.\n",
      "Evaluated on 600 sequences.\n",
      "Evaluated on 700 sequences.\n",
      "Evaluated on 800 sequences.\n",
      "Evaluated on 900 sequences.\n",
      "Evaluated on 1000 sequences.\n",
      "Evaluated on 1100 sequences.\n",
      "Full-precision model accuracy is 97.6130% (Number of test samples=1173)\n",
      "Quantized model accuracy is 97.1014% (Number of test samples=1173)\n"
     ]
    }
   ],
   "source": [
    "# we plot also the confusion matrix of the quantized model\n",
    "tflite_model_quant_int8_pred = run_tflite_model(tflite_model_quant_int8_file, range(y_test.shape[0]))\n",
    "\n",
    "# compute the accuracy of the quantized model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "full_int8_accuracy = accuracy_score(labels, tflite_model_quant_int8_pred)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (test_accuracy * 100, len(y_test)))\n",
    "print(\"Quantized model accuracy is %.4f%% (Number of test samples=%d)\" % (full_int8_accuracy * 100, len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a86b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07ec44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'q8_demo_cnn'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2690e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image data location:  /home/amroset/Machine Learning on Microcontrollers/Project/x_test_gestures_demo.npy\n",
      "Test labels location:  /home/amroset/Machine Learning on Microcontrollers/Project/y_test_gestures_demo.npy\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "\n",
    "# save the test data as numpy arrays\n",
    "np.save('x_test_gestures_demo.npy', (X_test / input_scale + input_zero_point).astype(np.uint8))\n",
    "np.save('y_test_gestures_demo.npy', (y_test.astype(np.uint8)))\n",
    "\n",
    "# print the location of the files\n",
    "print('Test image data location: ', os.path.abspath('x_test_gestures_demo.npy'))\n",
    "print('Test labels location: ', os.path.abspath('y_test_gestures_demo.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f882216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "import gc\n",
    "\n",
    "def get_cnn_quantized_model(num_classes):\n",
    "\n",
    "        gc.collect()\n",
    "        keras.backend.clear_session()\n",
    "    \n",
    "    \n",
    "        #qkeras model\n",
    "\n",
    "        quantized_model = keras.Sequential([\n",
    "                        keras.layers.Input((32,3)),\n",
    "                        keras.layers.Reshape((8,4,3)),\n",
    "                        #QActivation(\"quantized_bits(16)\"),\n",
    "                        QConv2D(filters=8, kernel_size = (5,5), padding = \"same\", kernel_quantizer=\"quantized_bits(8)\"),\n",
    "                        keras.layers.BatchNormalization(),\n",
    "                        QActivation(\"quantized_relu(8)\"),\n",
    "                        QConv2D(filters=8, kernel_size = (5,5), padding = \"same\", kernel_quantizer=\"quantized_bits(8)\"),\n",
    "                        keras.layers.BatchNormalization(),\n",
    "                        QActivation(\"quantized_relu(8)\"),\n",
    "                        keras.layers.MaxPool2D(pool_size = 2),\n",
    "                        keras.layers.Flatten(),\n",
    "                        QDense(32, kernel_quantizer=\"quantized_bits(8)\"),\n",
    "                        keras.layers.BatchNormalization(),\n",
    "                        QActivation(\"quantized_relu(8)\"),\n",
    "                        QDense(num_classes, kernel_quantizer=\"quantized_bits(8)\"),\n",
    "                        keras.layers.Activation('softmax'),\n",
    "                        ])\n",
    "\n",
    "        return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f38861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 8, 4, 3)           0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 8, 4, 8)           608       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 8, 4, 8)           32        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 8, 4, 8)           0         \n",
      "                                                                 \n",
      " q_conv2d_1 (QConv2D)        (None, 8, 4, 8)           1608      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 8, 4, 8)           32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 8, 4, 8)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 4, 2, 8)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 8)                 264       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4752 (18.56 KB)\n",
      "Trainable params: 4656 (18.19 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qmodel = get_cnn_quantized_model(len(label_encoder.classes_))\n",
    "\n",
    "qmodel.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "qmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e45cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 2s 6ms/step - loss: 1.0844 - accuracy: 0.7501 - val_loss: 0.8365 - val_accuracy: 0.8679 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.9380 - val_loss: 0.4233 - val_accuracy: 0.9437 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.9642 - val_loss: 0.2483 - val_accuracy: 0.9659 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.9736 - val_loss: 0.1715 - val_accuracy: 0.9787 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9824 - val_loss: 0.1283 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9898 - val_loss: 0.1000 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9912 - val_loss: 0.0828 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9946 - val_loss: 0.0627 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0462 - accuracy: 0.9943 - val_loss: 0.0551 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.9980 - val_loss: 0.0468 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9983 - val_loss: 0.0405 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9986 - val_loss: 0.0415 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9986 - val_loss: 0.0340 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9986 - val_loss: 0.0347 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9997 - val_loss: 0.0293 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9994 - val_loss: 0.0310 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9997 - val_loss: 0.0279 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9997 - val_loss: 0.0265 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9997 - val_loss: 0.0270 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.0272 - val_accuracy: 0.9923 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x747534103460>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate the quantization aware model\n",
    "es = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_delta=0.0001, mode='auto', cooldown=0, min_lr=0)\n",
    "                \n",
    "    ]\n",
    "qmodel.fit(\n",
    "                  X_train,\n",
    "                  y_train,\n",
    "                  epochs=20,\n",
    "                  validation_data=(X_validation, y_validation),\n",
    "                  callbacks=[es]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4afabf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.021411869674921036\n",
      "Quantization aware training accuracy:  0.9957374334335327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/keras/src/constraints.py:365: UserWarning: The `keras.constraints.serialize()` API should only be used for objects of type `keras.constraints.Constraint`. Found an instance of type <class 'qkeras.quantizers.quantized_bits'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = qmodel.evaluate(X_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "qmodel.save('qat_cnn_gesture_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d21863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm5c9q6s5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm5c9q6s5/assets\n",
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-02 17:26:04.521530: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-02 17:26:04.521547: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-02 17:26:04.521644: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpm5c9q6s5\n",
      "2025-01-02 17:26:04.524809: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-02 17:26:04.524819: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpm5c9q6s5\n",
      "2025-01-02 17:26:04.533766: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-02 17:26:04.595482: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpm5c9q6s5\n",
      "2025-01-02 17:26:04.622605: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 100958 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 20, Total Ops 79, % non-converted = 25.32 %\n",
      " * 20 ARITH ops\n",
      "\n",
      "- arith.constant:   20 occurrences  (f32: 14, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 6)\n",
      "  (f32: 3)\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (i1: 3)\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 8)\n",
      "  (i32: 1)\n",
      "  (f32: 5)\n",
      "  (f32: 2)\n",
      "  (f32: 3)\n",
      "  (f32: 3)\n",
      "  (i64: 3, i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 6)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(qmodel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab83f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n",
      "TFLite Model size with 8-bit quantization: 21 KB\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"models/demo_gesture_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('models/demo_gesture_qat_int8.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "528af573",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'demo_qat8_gesture'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cd0b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 sequences.\n",
      "Evaluated on 100 sequences.\n",
      "Evaluated on 200 sequences.\n",
      "Evaluated on 300 sequences.\n",
      "Evaluated on 400 sequences.\n",
      "Evaluated on 500 sequences.\n",
      "Evaluated on 600 sequences.\n",
      "Evaluated on 700 sequences.\n",
      "Evaluated on 800 sequences.\n",
      "Evaluated on 900 sequences.\n",
      "Evaluated on 1000 sequences.\n",
      "Evaluated on 1100 sequences.\n",
      "Full QAT INT8 model accuracy is 99.4032% (Number of test samples=1173)\n",
      "Full-precision model accuracy is 97.3572% (Number of test samples=3518)\n"
     ]
    }
   ],
   "source": [
    "# this might take a few minutes (~ 1- 2 minutes)\n",
    "# if it takes longer than that, I suggest to \n",
    "# restart the runtime and try again\n",
    "# if the issue still persists, restart your computer\n",
    "tflite_model_quant_int8_qat_file = pathlib.Path('models/demo_gesture_qat_int8.tflite')\n",
    "tflite_model_quant_int8_qat_type = \"Full QAT INT8\"\n",
    "\n",
    "evaluate_model(tflite_model_quant_int8_qat_file, tflite_model_quant_int8_qat_type)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (test_accuracy * 100, len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9981a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 sequences.\n",
      "Evaluated on 100 sequences.\n",
      "Evaluated on 200 sequences.\n",
      "Evaluated on 300 sequences.\n",
      "Evaluated on 400 sequences.\n",
      "Evaluated on 500 sequences.\n",
      "Evaluated on 600 sequences.\n",
      "Evaluated on 700 sequences.\n",
      "Evaluated on 800 sequences.\n",
      "Evaluated on 900 sequences.\n",
      "Evaluated on 1000 sequences.\n",
      "Evaluated on 1100 sequences.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "tflite_model_quant_int8_qat_file = \"models/demo_gesture_qat_int8.tflite\"\n",
    "tflite_model_quant_int8_qat_model_type = \"Quantized aware training model\"\n",
    "tflite_model_quant_int8_qat_pred = run_tflite_model(tflite_model_quant_int8_qat_file, range(X_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad4ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full QAT INT8 accuracy is 99.4032% (Number of test samples=1173)\n",
      "Full-precision model accuracy is 97.3572% (Number of test samples=1173)\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy of the model\n",
    "full_qat_int8_accuracy = accuracy_score(labels, tflite_model_quant_int8_qat_pred)\n",
    "print('Full QAT INT8 accuracy is %.4f%% (Number of test samples=%d)' % (full_qat_int8_accuracy * 100, len(y_test)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (test_accuracy * 100, len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355dd94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
