{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a5fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:20:55.389943: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-10 15:20:55.411217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-10 15:20:55.411239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-10 15:20:55.411805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-10 15:20:55.415892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-10 15:20:55.820794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv1d  (None, 32, 21)            401       \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 32, 21)            1         \n",
      " t_9 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 32, 21)            85        \n",
      " normalization_9 (PruneLowM                                      \n",
      " agnitude)                                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_  (None, 32, 21)            1         \n",
      " 9 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_reshap  (None, 16, 14, 3)         1         \n",
      " e_3 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 16, 14, 8)         442       \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 16, 14, 8)         1         \n",
      " t_10 (PruneLowMagnitude)                                        \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 16, 14, 8)         33        \n",
      " normalization_10 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_  (None, 16, 14, 8)         1         \n",
      " 10 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 8, 7, 8)           1         \n",
      " oling2d_3 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 448)               1         \n",
      " n_3 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 20)                17942     \n",
      " 6 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 20)                1         \n",
      " t_11 (PruneLowMagnitude)                                        \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 20)                81        \n",
      " normalization_11 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_  (None, 20)                1         \n",
      " 11 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 20)                822       \n",
      " 7 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19815 (77.46 KB)\n",
      "Trainable params: 9932 (38.80 KB)\n",
      "Non-trainable params: 9883 (38.67 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.saving.load_model('models/tcn_gesture_classification_model.h5')\n",
    "\n",
    "#OPTIONAL: prune the model before quantization\n",
    "\n",
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.2, begin_step=200, frequency=50),\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b96a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/214 [==============================] - 1s 6ms/step - loss: 0.6754 - accuracy: 0.7620 - val_loss: 0.2681 - val_accuracy: 0.9351 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.7237 - accuracy: 0.7477 - val_loss: 0.1718 - val_accuracy: 0.9715 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.7607 - val_loss: 0.1618 - val_accuracy: 0.9667 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6862 - accuracy: 0.7686 - val_loss: 0.1654 - val_accuracy: 0.9715 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.7728 - val_loss: 0.1739 - val_accuracy: 0.9719 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.7760 - val_loss: 0.1578 - val_accuracy: 0.9689 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.7738 - val_loss: 0.1654 - val_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.7658 - val_loss: 0.1622 - val_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.7737 - val_loss: 0.1531 - val_accuracy: 0.9737 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.7741 - val_loss: 0.1571 - val_accuracy: 0.9680 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7a7882b59f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "es = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_delta=0.0001, mode='auto', cooldown=0, min_lr=0),\n",
    "        tfmot.sparsity.keras.UpdatePruningStep()\n",
    "               \n",
    "    ]\n",
    "# Train and evaluate the pruned model\n",
    "pruned_model_unstructured.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_validation, y_validation),\n",
    "                    callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c760d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUANTIZATION STARTS HERE\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "model = tf.keras.saving.load_model(\"models/tcn_gesture_classification_model.h5\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Annotate the model for QAT, skipping BatchNorm.\n",
    "def apply_quantization(layer):\n",
    "    # Skip quantization for unsupported layers\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        return layer\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "\n",
    "# Clone the model and apply the quantization annotation\n",
    "#annotated_model = tf.keras.models.clone_model(model, clone_function=apply_quantization)\n",
    " \n",
    "# Apply Quantization Aware Training (QAT)\n",
    "qat_model = tfmot.quantization.keras.quantize_annotate_model(model) \n",
    "#qat_model = tfmot.quantization.keras.quantize(annotated_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf8a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_annotate (Quantiz  (None, 32, 21)            210       \n",
      " eAnnotate)                                                      \n",
      "                                                                 \n",
      " quantize_annotate_1 (Quant  (None, 32, 21)            0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_2 (Quant  (None, 32, 21)            84        \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_3 (Quant  (None, 32, 21)            0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_4 (Quant  (None, 16, 14, 3)         0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_5 (Quant  (None, 16, 14, 8)         224       \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_6 (Quant  (None, 16, 14, 8)         0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_7 (Quant  (None, 16, 14, 8)         32        \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_8 (Quant  (None, 16, 14, 8)         0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_9 (Quant  (None, 8, 7, 8)           0         \n",
      " izeAnnotate)                                                    \n",
      "                                                                 \n",
      " quantize_annotate_10 (Quan  (None, 448)               0         \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      " quantize_annotate_11 (Quan  (None, 20)                8980      \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      " quantize_annotate_12 (Quan  (None, 20)                0         \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      " quantize_annotate_13 (Quan  (None, 20)                80        \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      " quantize_annotate_14 (Quan  (None, 20)                0         \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      " quantize_annotate_15 (Quan  (None, 20)                420       \n",
      " tizeAnnotate)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10030 (39.18 KB)\n",
      "Trainable params: 9932 (38.80 KB)\n",
      "Non-trainable params: 98 (392.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qat_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "qat_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3556e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FROM ORIGINAL DATASET: to use if training models for evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the data\n",
    "# Replace 'train.csv' with the actual path to your dataset\n",
    "data = pd.read_csv('dataset/train.csv', header = None, converters = {\n",
    "    3: ast.literal_eval,\n",
    "    4: ast.literal_eval,\n",
    "    5: ast.literal_eval\n",
    "}, skiprows = 1)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['acc_x'] = data[3]\n",
    "df['acc_y'] = data[4]\n",
    "df['acc_z'] = data[5]\n",
    "df['gesture'] = data[2]\n",
    "\n",
    "#remove invalid rows\n",
    "df.drop(df.loc[df['acc_x']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_y']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_z']==0].index, inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert the lists into arrays\n",
    "acc_x = df['acc_x'].values\n",
    "acc_y = df['acc_y'].values\n",
    "acc_z = df['acc_z'].values\n",
    "\n",
    "# Combine all axes into a sequence of shape (timesteps, features)\n",
    "sequences = [np.array([x, y, z]).T for x, y, z in zip(acc_x, acc_y, acc_z)]\n",
    "\n",
    "# Pad sequences to the length of the longest sequence\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 32, padding='post', dtype='float32')\n",
    "\n",
    "# Encode labels\n",
    "labels = df['gesture'].values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63920f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FROM MY PERSONAL DATASET: to use if training models for demo\n",
    "\n",
    "# Load and preprocess the data\n",
    "# Replace 'train.csv' with the actual path to your dataset\n",
    "data = pd.read_csv('dataset/combined_shuffled.csv', header = None, converters = {\n",
    "    1: ast.literal_eval,\n",
    "    2: ast.literal_eval,\n",
    "    3: ast.literal_eval\n",
    "})\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['acc_x'] = data[1]\n",
    "df['acc_y'] = data[2]\n",
    "df['acc_z'] = data[3]\n",
    "df['gesture'] = data[0]\n",
    "\n",
    "#remove invalid rows\n",
    "df.drop(df.loc[df['acc_x']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_y']==0].index, inplace=True)\n",
    "df.drop(df.loc[df['acc_z']==0].index, inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert the lists into arrays\n",
    "acc_x = df['acc_x'].values\n",
    "acc_y = df['acc_y'].values\n",
    "acc_z = df['acc_z'].values\n",
    "\n",
    "# Combine all axes into a sequence of shape (timesteps, features)\n",
    "sequences = [np.array([x, y, z]).T for x, y, z in zip(acc_x, acc_y, acc_z)]\n",
    "\n",
    "# Pad sequences to the length of the longest sequence\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 32, padding='post', dtype='float32')\n",
    "\n",
    "# Encode labels\n",
    "labels = df['gesture'].values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_labels = categorical_labels\n",
    "initial_sequences = padded_sequences\n",
    "\n",
    "#DATA AUGMENTATION preprocessing\n",
    "\n",
    "#adding noise\n",
    "def add_noise(data, noise_level=0.05):\n",
    "    return (data + np.random.normal(0, noise_level, data.shape)).astype(np.float32)\n",
    "\n",
    "# Original data: `x_train` (accelerometer sequences), `y_train` (labels)\n",
    "\n",
    "noised_data = add_noise(initial_sequences)\n",
    "\n",
    "def scale_data(data, scaling_factor=0.1):\n",
    "    \n",
    "    factor = 1 + np.random.uniform(-scaling_factor, scaling_factor)\n",
    "    return (data * factor).astype(np.float32)\n",
    "\n",
    "def time_shift(data, max_shift_percentage=0.1):\n",
    "  \n",
    "  shift_amount = int(len(data) * max_shift_percentage * (2 * np.random.rand() - 1))\n",
    "  shifted_data = np.roll(data, shift_amount, axis=0)\n",
    "  return (shifted_data).astype(np.float32)\n",
    "\n",
    "#categorical_labels = np.concatenate((categorical_labels, initial_labels))\n",
    "\n",
    "\"\"\"def time_warp(data, warping_factor=0.1):\n",
    "    \n",
    "  timesteps = np.arange(len(data))\n",
    "  warped_timesteps = timesteps + warping_factor * (np.random.rand(len(data)) - 0.5) * len(data)\n",
    "  warped_timesteps = np.interp(np.arange(len(data)), np.sort(warped_timesteps), timesteps)\n",
    "  warped_data = np.zeros_like(data)\n",
    "  for i in range(len(data)):\n",
    "    warped_data[i] = np.interp(i, timesteps, data)\n",
    "  return warped_data\n",
    "  \"\"\"\n",
    "\n",
    "#warped_seq = time_warp(padded_sequences)\n",
    "\n",
    "def rotate_3d(data, max_angle_degrees=10):\n",
    "\n",
    "  angle_x = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "  angle_y = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "  angle_z = np.random.uniform(-max_angle_degrees, max_angle_degrees) * np.pi / 180\n",
    "\n",
    "  rotation_x = np.array([[1, 0, 0],\n",
    "                        [0, np.cos(angle_x), -np.sin(angle_x)],\n",
    "                        [0, np.sin(angle_x), np.cos(angle_x)]])\n",
    "  rotation_y = np.array([[np.cos(angle_y), 0, np.sin(angle_y)],\n",
    "                        [0, 1, 0],\n",
    "                        [-np.sin(angle_y), 0, np.cos(angle_y)]])\n",
    "  rotation_z = np.array([[np.cos(angle_z), -np.sin(angle_z), 0],\n",
    "                        [np.sin(angle_z), np.cos(angle_z), 0],\n",
    "                        [0, 0, 1]])\n",
    "\n",
    "  rotation_matrix = np.dot(np.dot(rotation_x, rotation_y), rotation_z)\n",
    "  rotated_data = np.dot(data, rotation_matrix.T)\n",
    "\n",
    "  return (rotated_data).astype(np.float32)\n",
    "\n",
    "rotated_seq = rotate_3d(initial_sequences)\n",
    "\n",
    "padded_sequences = np.concatenate((padded_sequences, add_noise(padded_sequences)))\n",
    "categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "#padded_sequences = np.concatenate((padded_sequences, scale_data(padded_sequences)))\n",
    "#categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "#padded_sequences = np.concatenate((padded_sequences, time_shift(padded_sequences)))\n",
    "#categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n",
    "\n",
    "\n",
    "padded_sequences = np.concatenate((padded_sequences, rotate_3d(padded_sequences)))\n",
    "categorical_labels = np.concatenate((categorical_labels, categorical_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b22076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    padded_sequences, categorical_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f006b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/214 [==============================] - 7s 4ms/step - loss: 0.6118 - accuracy: 0.7939 - val_loss: 0.1362 - val_accuracy: 0.9732\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.7947 - val_loss: 0.1377 - val_accuracy: 0.9697\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.7905 - val_loss: 0.1493 - val_accuracy: 0.9697\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.7933 - val_loss: 0.1407 - val_accuracy: 0.9671\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.7977 - val_loss: 0.1363 - val_accuracy: 0.9711\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5900 - accuracy: 0.7984 - val_loss: 0.1394 - val_accuracy: 0.9702\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5967 - accuracy: 0.7982 - val_loss: 0.1352 - val_accuracy: 0.9697\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5865 - accuracy: 0.7985 - val_loss: 0.1387 - val_accuracy: 0.9702\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5747 - accuracy: 0.8054 - val_loss: 0.1324 - val_accuracy: 0.9702\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5873 - accuracy: 0.8022 - val_loss: 0.1329 - val_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Train using model.fit\n",
    "\n",
    "# Train the model\n",
    "history = qat_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9dcccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.06390826404094696\n",
      "Quantization aware training accuracy:  0.9864035248756409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = qat_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "qat_model.save('pruned_qat_tcn_gesture_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e435091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpr35m44ur/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpr35m44ur/assets\n",
      "/home/amroset/anaconda3/envs/microcontrollers/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-04 15:10:33.401337: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-04 15:10:33.401363: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-04 15:10:33.401486: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpr35m44ur\n",
      "2025-01-04 15:10:33.406868: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-04 15:10:33.406889: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpr35m44ur\n",
      "2025-01-04 15:10:33.421514: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-04 15:10:33.496221: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpr35m44ur\n",
      "2025-01-04 15:10:33.523795: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 122307 microseconds.\n",
      "2025-01-04 15:10:33.677618: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677654: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677656: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677657: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677662: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677664: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677665: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2025-01-04 15:10:33.677666: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 30, Total Ops 89, % non-converted = 33.71 %\n",
      " * 29 ARITH ops, 1 TF_SAVED_MODEL ops\n",
      "\n",
      "- arith.constant:   29 occurrences  (f32: 18, i32: 11)\n",
      "\n",
      "\n",
      "\n",
      "- tf_saved_model.session_initializer:    1 occurrences\n",
      "  (f32: 5)\n",
      "\n",
      "  (f32: 2)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 7)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 8)\n",
      "  (f32: 4)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (: 8)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfac8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n",
      "TFLite Model size with 8-bit quantization: 53 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"models/gesture_tcn_pruned_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('models/gesture_tcn_pruned_qat_int8.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee95edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b203fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'qat8_gesture'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(qat_model, c_model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
